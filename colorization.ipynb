{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGvAocDyT8E3"
   },
   "source": [
    "# Libraries\n",
    "Requirement for the execution of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnXAaKJleBU-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "from skimage import color\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PhDQT0tloMk"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "\n",
    "from torchmetrics import functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZiKmLYA2YWF"
   },
   "source": [
    "# Hyper Parameters\n",
    "we can tune this parameters to modify the training proces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kx4rsuz48t0p"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 2e-4\n",
    "batch_size = 16\n",
    "n_workers = 2\n",
    "img_size = 256\n",
    "L1_lambda = 100\n",
    "n_epochs = 100\n",
    "n_samples = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ess0aS-T2bo7"
   },
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d5j56ImeQfh"
   },
   "outputs": [],
   "source": [
    "class PatchGAN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels = 4, out_channels = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        features = [64, 128, 256, 512]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect'),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features[0], features[1], kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect'),\n",
    "            nn.BatchNorm2d(features[1]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features[1], features[2], kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect'),\n",
    "            nn.BatchNorm2d(features[2]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features[2], features[3], kernel_size = 4, stride = 1, padding = 1, padding_mode = 'reflect'),\n",
    "            nn.BatchNorm2d(features[3]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features[3], out_channels, kernel_size = 4, stride = 1, padding = 1, padding_mode = 'reflect')\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], dim = 1) # we are using a conditional GAN so the input of the discriminator must include the input of the generator\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tj13zfA4pFRW"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels = 4, out_channels = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        features = [64, 128, 256, 512]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect'),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features[0], features[1], kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect'),\n",
    "            nn.InstanceNorm2d(features[1], affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features[1], features[2], kernel_size = 4, stride = 2, padding = 1, padding_mode = 'reflect'),\n",
    "            nn.InstanceNorm2d(features[2], affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features[2], features[3], kernel_size = 4, stride = 1, padding = 1, padding_mode = 'reflect'),\n",
    "            nn.InstanceNorm2d(features[3], affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features[3], out_channels, kernel_size = 4, stride = 1, padding = 1, padding_mode = 'reflect')\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], dim = 1) # we are using a conditional GAN so the input of the discriminator must include the input of the generator\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJnpt-VIiPX8"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels = 1, out_channels = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pool_layer = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.encoder_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channels, out_channels = 64, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 64),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.encoder_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.encoder_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.encoder_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.middle_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 1024, out_channels = 1024, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.upsampling_block_4 = nn.ConvTranspose2d(in_channels = 1024, out_channels = 512, kernel_size=2, stride=2)\n",
    "\n",
    "        self.decoder_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 512 + 512, out_channels = 512, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.upsampling_block_3 = nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size=2, stride=2)\n",
    "\n",
    "        self.decoder_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 256 + 256, out_channels = 256, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.upsampling_block_2 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size=2, stride=2)\n",
    "\n",
    "        self.decoder_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 128 + 128, out_channels = 128, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.upsampling_block_1 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.decoder_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 64 + 64, out_channels = 64, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(num_features = 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.final_block = nn.Conv2d(in_channels = 64, out_channels = out_channels, kernel_size = 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder_block_1(x)\n",
    "        enc2 = self.encoder_block_2(self.pool_layer(enc1))\n",
    "        enc3 = self.encoder_block_3(self.pool_layer(enc2))\n",
    "        enc4 = self.encoder_block_4(self.pool_layer(enc3))\n",
    "\n",
    "        mid = self.middle_block(self.pool_layer(enc4))\n",
    "\n",
    "        dec4 = self.upsampling_block_4(mid)\n",
    "        dec4 = torch.cat([dec4, enc4], dim=1) # skip connection\n",
    "        dec4 = self.decoder_block_4(dec4)\n",
    "\n",
    "        dec3 = self.upsampling_block_3(dec4)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1) # skip connection\n",
    "        dec3 = self.decoder_block_3(dec3)\n",
    "\n",
    "        dec2 = self.upsampling_block_2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1) # skip connection\n",
    "        dec2 = self.decoder_block_2(dec2)\n",
    "\n",
    "        dec1 = self.upsampling_block_1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1) # skip connection\n",
    "        dec1 = self.decoder_block_1(dec1)\n",
    "\n",
    "        return torch.sigmoid(self.final_block(dec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLbICYUL2h4M"
   },
   "source": [
    "# Obtaining the Coco Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yjsa4eXmmgaf"
   },
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/zips/train2017.zip -O \"/content/train2017.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJFlkquGmI6o"
   },
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile('/content/train2017.zip', 'r')\n",
    "zip_ref.extractall('/content')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAsD1q9i2nub"
   },
   "source": [
    "# Defining our Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAT486-k7pPY"
   },
   "outputs": [],
   "source": [
    "path = \"/content/train2017\"\n",
    "paths = glob.glob(path + \"/*.jpg\") # Creates a list with the paths of all the images of the training set\n",
    "np.random.seed(2052415)\n",
    "\n",
    "paths_subset = np.random.choice(paths, n_samples, replace = False)\n",
    "rand = np.random.permutation(n_samples)\n",
    "train_indexes = rand[:(int)(n_samples * 0.8)] \n",
    "test_indexes = rand[(int)(n_samples * 0.8):] \n",
    "\n",
    "train_paths = paths_subset[train_indexes] # Contains the selected number of training image paths\n",
    "test_paths = paths_subset[test_indexes] # Contains the selected number of training image paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsR33buPhrJQ"
   },
   "source": [
    "We can use GrayRGB to have a gray image to colorize and its ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBKaFGoe71w2"
   },
   "outputs": [],
   "source": [
    "class GrayRGB(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.size = img_size\n",
    "        self.paths = paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.paths[index]).convert(\"RGB\")\n",
    "        img = transforms.Resize((self.size, self.size))(img)\n",
    "        img = np.array(img).astype(\"float32\") / 255 # values in [0, 1]\n",
    "       \n",
    "        img_gray = color.rgb2gray(img).astype(\"float32\")\n",
    "        img_gray = transforms.ToTensor()(img_gray)\n",
    "        img_rgb = transforms.ToTensor()(img) \n",
    "        \n",
    "        return {\"gray\" : img_gray, \"rgb\" : img_rgb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "karNE7cwi7jz"
   },
   "source": [
    "We can use labRGB to have a Luminance channel and its corresponding a and b channels that together form the image in Lab colorspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5W4FjfBvzQ4I"
   },
   "outputs": [],
   "source": [
    "class LabRGB(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.size = img_size\n",
    "        self.paths = paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.paths[index]).convert(\"RGB\")\n",
    "        img = transforms.Resize((self.size, self.size))(img)\n",
    "        img = np.array(img)\n",
    "        img_Lab = color.rgb2lab(img).astype(\"float32\")\n",
    "        img_L = img_Lab[:, :, :1] / 100 # values in [0, 1]\n",
    "        img_ab = (img_Lab[:, :, 1:] + 128) / 255 # values in [0, 1]\n",
    "        img_L = transforms.ToTensor()(img_L)\n",
    "        img_ab = transforms.ToTensor()(img_ab)\n",
    "        \n",
    "        return {\"L\" : img_L, \"ab\" : img_ab}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJJl4xSQA4vV"
   },
   "source": [
    "# Visualization Functions\n",
    "Function needed to visualize some example during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXgOh1f-A5KT"
   },
   "outputs": [],
   "source": [
    "def visualize(x, y, y_fake, i, n_batch, lab = False):\n",
    "\n",
    "    fig = plt.figure(figsize=(9, 4))\n",
    "\n",
    "    plt.suptitle(f\"\\nIteration {i}/{n_batch}\")\n",
    "\n",
    "    if not lab:\n",
    "        ax = plt.subplot(1, 3, 1)\n",
    "        ax.imshow(x[0].cpu().detach().numpy().astype('float32'), cmap='gray')\n",
    "        ax.set_title(\"Grayscale Image\")\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(1, 3, 2)\n",
    "        ax.imshow(y_fake.cpu().detach().permute(1,2,0).numpy().astype('float32'))\n",
    "        ax.set_title(\"RGB Generated Image\")\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(1, 3, 3)\n",
    "        ax.imshow(y.cpu().detach().permute(1,2,0).numpy().astype('float32'))\n",
    "        ax.set_title(\"RGB Real Image\")\n",
    "        ax.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        x = x * 100\n",
    "        y = y * 255 - 128\n",
    "        y_fake = y_fake * 255 - 128\n",
    "        ax = plt.subplot(1, 3, 1)\n",
    "        lab_fake = torch.cat([x, y_fake], dim = 0).permute(1, 2, 0)\n",
    "        rgb_fake = color.lab2rgb(lab_fake.cpu().detach())\n",
    "        lab_real = torch.cat([x, y], dim = 0).permute(1, 2, 0)\n",
    "        rgb_real = color.lab2rgb(lab_real.cpu().detach())\n",
    "        ax.imshow(x[0].cpu().detach().numpy().astype('float32'), cmap='gray')\n",
    "        ax.set_title(\"Grayscale Image\")\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(1, 3, 2)\n",
    "        ax.imshow(rgb_fake)\n",
    "        ax.set_title(\"RGB Generated Image\")\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(1, 3, 3)\n",
    "        ax.imshow(rgb_real)\n",
    "        ax.set_title(\"RGB Real Image\")\n",
    "        ax.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3QwmoIq2xNV"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTOKVHcyaHuJ"
   },
   "source": [
    "## Training functions\n",
    "We optimize our network with this fucntion, thanks to its flags it can satisfy various cases according to our needs\n",
    "- ssim_loss: if True ssim metrics is used to improve the convergence of the generator\n",
    "- lab: if True the training is performed in the Lab colorspace instead of the standard RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bY2evsndLMH9"
   },
   "outputs": [],
   "source": [
    "def train_step(disc, gen, dl, opt_disc, opt_gen, l1, BCE, d_scaler, g_scaler, ssim_loss = False, lab = False, display_every = 50):\n",
    "    i = 0\n",
    "    loop = tqdm(dl, leave = True)\n",
    "\n",
    "    for data in loop:\n",
    "\n",
    "        if not lab:\n",
    "            x = data[\"gray\"]\n",
    "            y = data[\"rgb\"]\n",
    "        else:\n",
    "            x = data[\"L\"]\n",
    "            y = data[\"ab\"]\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Discriminator train\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            y_fake = gen(x)\n",
    "            pred_real = disc(x, y)\n",
    "            pred_fake = disc(x, y_fake.detach())\n",
    "            disc_real_loss = BCE(pred_real, torch.ones_like(pred_real))\n",
    "            disc_fake_loss = BCE(pred_fake, torch.zeros_like(pred_fake))\n",
    "            disc_loss = (disc_real_loss + disc_fake_loss) / 2\n",
    "\n",
    "        disc.zero_grad()\n",
    "        d_scaler.scale(disc_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Generator train\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred_fake = disc(x, y_fake)\n",
    "            gen_fake_loss = BCE(pred_fake, torch.ones_like(pred_fake))\n",
    "            L1 = l1(y_fake, y) * L1_lambda\n",
    "            gen_loss = gen_fake_loss + L1\n",
    "            if ssim_loss:\n",
    "                ssim_value = functional.structural_similarity_index_measure(preds = y_fake, target = y)\n",
    "                gen_loss += (1 - ssim_value)\n",
    "\n",
    "        gen.zero_grad()\n",
    "        g_scaler.scale(gen_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "        if i % display_every == 0:\n",
    "            visualize(x[0], y[0], y_fake[0], i, len(dl), lab)\n",
    "            print(f\"\\nGenerator Loss: {gen_loss} \\nDiscriminator Loss: {disc_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNKNv731BjMN"
   },
   "outputs": [],
   "source": [
    "def train(discriminator, generator, device, dataset, learning_rate, batch_size, n_workers, n_epochs, ssim_loss = False, lab = False):\n",
    "\n",
    "    beta1 = 0.5\n",
    "    beta2 = 0.999\n",
    "\n",
    "    opt_disc = optim.Adam(discriminator.parameters(), lr = learning_rate, betas = (beta1, beta2))\n",
    "    opt_gen = optim.Adam(generator.parameters(), lr = learning_rate, betas = (beta1, beta2))\n",
    "    \n",
    "    L1_loss = nn.L1Loss()\n",
    "    BCE = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_dl = DataLoader(dataset, batch_size = batch_size, shuffle = True, num_workers = n_workers)\n",
    "\n",
    "    d_scaler = torch.cuda.amp.GradScaler()\n",
    "    g_scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "            train_step(discriminator, generator, train_dl, opt_disc, opt_gen, L1_loss, BCE, d_scaler, g_scaler, ssim_loss, lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbepd3d1nUkx"
   },
   "source": [
    "## Training with grayscale to RGB\n",
    "\n",
    "Here we are generating an RGB image (3 color channels) from a grayscale image (1 channel). Our network is a conditional GAN so the input of our discriminator is 4 (3 for RGB image and 1 for the grayscale image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeakBnALkEY0"
   },
   "outputs": [],
   "source": [
    "GrayRGB_dataset = GrayRGB(train_paths)\n",
    "\n",
    "gray_discriminator = PatchGAN(in_channels = 4).to(device)\n",
    "gray_generator = UNet(out_channels = 3).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "train(gray_discriminator, gray_generator, device, GrayRGB_dataset, lr, batch_size, n_workers, n_epochs)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Training time: %.2f s\" % elapsed_time)\n",
    "\n",
    "# Save the weights\n",
    "torch.save(gray_generator.state_dict(), '/content/gray_generator_weights.pt')\n",
    "torch.save(gray_discriminator.state_dict(), '/content/gray_discriminator_weights.pt')\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsSKnb-v0Hb-"
   },
   "source": [
    "## Training with Lab to RGB\n",
    "We are generating an RGB image from Lab color space, exploiting the fact that channel L represents the luminance and we can use it as a sort of grayscale image. This will let us generate only 2 channels (a and b) to recreate the final RGB image (with a conversion from Lab to RGB). This will benefit the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cdmo6eSc0HAZ"
   },
   "outputs": [],
   "source": [
    "LabRGB_dataset = LabRGB(train_paths)\n",
    "\n",
    "lab_discriminator = PatchGAN(in_channels = 3).to(device)\n",
    "lab_generator = UNet(out_channels = 2).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "train(lab_discriminator, lab_generator, device, LabRGB_dataset, lr, batch_size, n_workers, n_epochs, lab = True)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Training time: %.2f s\" % elapsed_time)\n",
    "\n",
    "# Save the weights\n",
    "torch.save(lab_generator.state_dict(), 'lab_generator_weights.pt')\n",
    "torch.save(lab_discriminator.state_dict(), '/content/lab_discriminator_weights.pt')\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y5GB7RxHq8M"
   },
   "source": [
    "## Training with Lab to RGB with SSIM Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3AlyXQ3HrO7"
   },
   "outputs": [],
   "source": [
    "ssim_lab_discriminator = PatchGAN(in_channels = 3).to(device)\n",
    "ssim_lab_generator = UNet(out_channels = 2).to(device)\n",
    "\n",
    "train(ssim_lab_discriminator, ssim_lab_generator, device, GrayRGB_dataset, lr, batch_size, n_workers, n_epochs, ssim_loss = True)\n",
    "\n",
    "# Save the weights\n",
    "torch.save(ssim_lab_generator.state_dict(), '/content/lab_generator_ssim_loss_weights.pt')\n",
    "torch.save(lab_discriminator.state_dict(), '/content/lab_discriminator_ssim_weights.pt')\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VABM4WUrryH-"
   },
   "source": [
    "## WGAN\n",
    "The main idea behind WGAN is to use a different loss function, the Wasserstein distance, which provides a more stable training process and improved generated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijpihFN-r6BO"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "\n",
    "def calculate_gradient_penalty(critic, device, real_images, gray_im, fake_images):\n",
    "    batch_size, channel, height, width = real_images.shape\n",
    "    eps_shape = [batch_size]+[1]*(len(real_images.shape)-1)\n",
    "    eps = torch.rand(eps_shape, device=device)\n",
    "    interpolated = eps * real_images + ((1 - eps) * fake_images)\n",
    "    # calculate probability of interpolated examples\n",
    "    score_interpolated = critic(gray_im, interpolated)\n",
    "\n",
    "    # calculate gradients of probabilities with respect to examples\n",
    "    gradients = autograd.grad(outputs=score_interpolated,\n",
    "                                inputs=interpolated,\n",
    "                                grad_outputs=torch.ones(score_interpolated.size(), device=device),\n",
    "                                create_graph=True,\n",
    "                                retain_graph=True,\n",
    "                                only_inputs=True,\n",
    "                                allow_unused=True)[0]\n",
    "    \n",
    "    grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return grad_penalty\n",
    "\n",
    "def W_train_step(critic, gen, device, dl, opt_critic, opt_gen, iter, lambda_term, lab = False):\n",
    "    i=1\n",
    "\n",
    "    for data in tqdm(dl):\n",
    "\n",
    "        if not lab:\n",
    "            gray = data[\"gray\"]\n",
    "            col = data[\"rgb\"]\n",
    "        else:\n",
    "            gray = data[\"L\"]\n",
    "            col = data[\"ab\"]\n",
    "\n",
    "        gray = gray.to(device)\n",
    "        col = col.to(device)\n",
    "\n",
    "        c_loss_real = 0\n",
    "        c_loss_fake = 0\n",
    "        \n",
    "        critic.zero_grad()\n",
    "        gen.zero_grad()\n",
    "    \n",
    "        # Train discriminator with real images\n",
    "        c_loss_real = critic(gray, col)\n",
    "\n",
    "        # Train with fake images\n",
    "        fake_images = gen(gray)\n",
    "        c_loss_fake = critic(gray, fake_images.detach())\n",
    "\n",
    "        # Train with gradient penalty\n",
    "        gradient_penalty = calculate_gradient_penalty(critic, device, col, gray, fake_images)\n",
    "\n",
    "        d_loss = (c_loss_fake - c_loss_real).mean() + (lambda_term*gradient_penalty.mean())\n",
    "        d_loss.backward()\n",
    "        opt_critic.step()\n",
    "\n",
    "        if(i%iter==0):\n",
    "            # Generator update\n",
    "            critic.zero_grad()\n",
    "            gen.zero_grad()\n",
    "            \n",
    "            # train generator\n",
    "            fake_images = gen(gray)\n",
    "            g_loss = -critic(gray, fake_images)\n",
    "            g_loss = g_loss.mean()\n",
    "            g_loss.backward()\n",
    "            opt_gen.step()\n",
    "\n",
    "        if(i==50):\n",
    "            print(f'\\nGenerator -> g_loss: {g_loss}')\n",
    "            print(f'Critic -> loss_fake: {c_loss_fake.mean()}, loss_real: {c_loss_real.mean()}')\n",
    "            visualize(gray[0], col[0], fake_images[0], i, len(dl), lab)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt7bdmvMxULt"
   },
   "outputs": [],
   "source": [
    "def W_train(critic, generator, device, dataset, learning_rate, batch_size, n_workers, n_epochs, lab=False):\n",
    "\n",
    "    beta1 = 0.5\n",
    "    beta2 = 0.999\n",
    "    iter = 5\n",
    "    lambda_term = 10\n",
    "\n",
    "    opt_critic = optim.Adam(critic.parameters(), lr = learning_rate, betas = (beta1, beta2))\n",
    "    opt_gen = optim.Adam(generator.parameters(), lr = learning_rate, betas = (beta1, beta2))\n",
    "\n",
    "    train_dl = DataLoader(dataset, batch_size = batch_size, shuffle = True, num_workers = n_workers)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "        W_train_step(critic, generator, device, train_dl, opt_critic, opt_gen, iter, lambda_term, lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_jsBcwW0Fjj"
   },
   "source": [
    "## WGAN training with grayscale to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFWrgxaTsLBX"
   },
   "outputs": [],
   "source": [
    "GrayRGB_dataset = GrayRGB(train_paths)\n",
    "\n",
    "wgan_gray_gen = UNet(out_channels = 3).to(device)\n",
    "wgan_gray_crit = Critic(in_channels = 4).to(device)\n",
    "\n",
    "W_train(wgan_gray_crit, wgan_gray_gen, device, GrayRGB_dataset, lr, batch_size, n_workers, n_epochs)\n",
    "\n",
    "# Save the weights\n",
    "torch.save(wgan_gray_gen.state_dict(), '/content/gray_generator_wgan_weights.pt')\n",
    "torch.save(wgan_gray_crit.state_dict(), '/content/gray_critic_wgan_weights.pt')\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hijIS-rK8U-b"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVF3PqFd8XmW"
   },
   "outputs": [],
   "source": [
    "def evaluate_network(dataloader, generator, discriminator, BCE, l1_loss, lab = False, wgan = False):\n",
    "    generator.eval()\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        input = []\n",
    "        generated = []\n",
    "        real = []\n",
    "        prediction_fake = []\n",
    "        for data in tqdm(dataloader):\n",
    "\n",
    "            if not lab:\n",
    "                x = data[\"gray\"]\n",
    "                y = data[\"rgb\"]\n",
    "            else:\n",
    "                x = data[\"L\"]\n",
    "                y = data[\"ab\"]\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_fake = generator(x)\n",
    "            pred_fake = discriminator(x, y_fake)\n",
    "\n",
    "            input.append(x)\n",
    "            generated.append(y_fake)\n",
    "            real.append(y)\n",
    "            prediction_fake.append(pred_fake)\n",
    "\n",
    "        input = torch.cat(input, axis=0)\n",
    "        generated = torch.cat(generated, axis=0)\n",
    "        real = torch.cat(real, axis=0)\n",
    "\n",
    "        prediction_fake = torch.cat(prediction_fake, axis=0)\n",
    "\n",
    "        ssim = functional.structural_similarity_index_measure(generated, real).item()\n",
    "\n",
    "        difference = generated - real\n",
    "        squared_difference = difference ** 2\n",
    "        mse = torch.mean(squared_difference).item()\n",
    "\n",
    "        if not wgan:\n",
    "            gen_fake_loss = BCE(prediction_fake, torch.ones_like(prediction_fake)).detach().cpu().numpy()\n",
    "            L1 = (l1_loss(generated, real) * L1_lambda).detach().cpu().numpy()\n",
    "            gen_loss = gen_fake_loss + L1\n",
    "        else:\n",
    "            gen_loss = -prediction_fake\n",
    "            gen_loss = gen_loss.mean()\n",
    "\n",
    "        print(f\"\\nGenerator Loss: {gen_loss}\")\n",
    "        print(f\"\\nMSE: {mse}\")\n",
    "        print(f\"\\nSSIM: {ssim}\")\n",
    "\n",
    "        if lab:\n",
    "            input = input * 100\n",
    "            real = real * 255 - 128\n",
    "            generated = generated * 255 - 128\n",
    "\n",
    "            lab_fake = torch.cat([input, generated], dim = 1).permute(0, 2, 3, 1)\n",
    "            generated = torch.tensor(color.lab2rgb(lab_fake.cpu())).permute(0, 3, 1, 2)\n",
    "            lab_real = torch.cat([input, real], dim = 1).permute(0, 2, 3, 1)\n",
    "            real = torch.tensor(color.lab2rgb(lab_real.cpu())).permute(0, 3, 1, 2)\n",
    "        \n",
    "        input = input.detach().cpu()\n",
    "        generated = generated.detach().cpu()\n",
    "        real = real.detach().cpu()\n",
    "\n",
    "        fig, axes = plt.subplots(3, 6, figsize=(15, 6))\n",
    "        axes = axes.ravel()\n",
    "        fig.suptitle(\"Generated images VS Real images\", fontsize = 15)\n",
    "        for i, ax in enumerate(axes):\n",
    "            if i % 6 < 3:\n",
    "                ax.imshow(generated[i].permute(1,2,0), aspect = 'equal')\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                ax.imshow(real[i - 3].permute(1,2,0), aspect = 'equal')\n",
    "                ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNNJ0VYs_Xwa"
   },
   "outputs": [],
   "source": [
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "L1_loss = nn.L1Loss()\n",
    "BCE = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dB-0erX49NJT"
   },
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "gray_generator_tester = UNet().to(device)\n",
    "gray_discriminator_tester = PatchGAN().to(device)\n",
    "\n",
    "gray_generator_tester.load_state_dict(torch.load('/content/gray_generator_weights.pt'))\n",
    "gray_discriminator_tester.load_state_dict(torch.load('/content/gray_discriminator_weights.pt'))\n",
    "\n",
    "opt_disc = optim.Adam(gray_discriminator_tester.parameters(), lr, betas = (beta1, beta2))\n",
    "opt_gen = optim.Adam(gray_generator_tester.parameters(), lr, betas = (beta1, beta2))\n",
    "\n",
    "gray_testing_ds = GrayRGB(test_paths)\n",
    "gray_testing_dl = DataLoader(gray_testing_ds, batch_size = 16, shuffle = True)\n",
    "\n",
    "print(\"From Grayscale to RGB\\n\")\n",
    "\n",
    "evaluate_network(gray_testing_dl, gray_generator_tester, gray_discriminator_tester, BCE, L1_loss)\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNnDsqaG-ZiN"
   },
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "lab_generator_tester = UNet(out_channels = 2).to(device)\n",
    "lab_discriminator_tester = PatchGAN(in_channels = 3).to(device)\n",
    "\n",
    "lab_generator_tester.load_state_dict(torch.load('/content/lab_generator_weights.pt'))\n",
    "lab_discriminator_tester.load_state_dict(torch.load('/content/lab_discriminator_weights.pt'))\n",
    "\n",
    "opt_disc = optim.Adam(lab_discriminator_tester.parameters(), lr, betas = (beta1, beta2))\n",
    "opt_gen = optim.Adam(lab_generator_tester.parameters(), lr, betas = (beta1, beta2))\n",
    "\n",
    "lab_testing_ds = LabRGB(test_paths)\n",
    "lab_testing_dl = DataLoader(lab_testing_ds, batch_size = 16, shuffle = True)\n",
    "\n",
    "print(\"From Lab to RGB\\n\")\n",
    "\n",
    "evaluate_network(lab_testing_dl, lab_generator_tester, lab_discriminator_tester, BCE, L1_loss, lab = True)\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LPj0QJ_Hm8Y"
   },
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "gray_generator_wgan_tester = UNet().to(device)\n",
    "gray_discriminator_wgan_tester = Critic().to(device)\n",
    "\n",
    "gray_generator_wgan_tester.load_state_dict(torch.load('/content/gray_generator_wgan_weights.pt'))\n",
    "gray_discriminator_wgan_tester.load_state_dict(torch.load('/content/gray_critic_wgan_weights.pt'))\n",
    "\n",
    "opt_disc = optim.Adam(gray_discriminator_wgan_tester.parameters(), lr, betas = (beta1, beta2))\n",
    "opt_gen = optim.Adam(gray_generator_wgan_tester.parameters(), lr, betas = (beta1, beta2))\n",
    "\n",
    "gray_testing_wgan_ds = GrayRGB(test_paths)\n",
    "gray_testing_wgan_dl = DataLoader(gray_testing_wgan_ds, batch_size = 16, shuffle = True)\n",
    "\n",
    "print(\"From Grayscale to RGB WGAN\\n\")\n",
    "\n",
    "evaluate_network(gray_testing_wgan_dl, gray_generator_wgan_tester, gray_discriminator_wgan_tester, BCE, L1_loss, wgan = True)\n",
    "\n",
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
